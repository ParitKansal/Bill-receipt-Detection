{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":702372,"sourceType":"datasetVersion","datasetId":358221}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/paritkansal/data-creation-object-detection?scriptVersionId=219458283\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install -q datasets pillow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:45:11.147641Z","iopub.execute_input":"2025-01-26T19:45:11.148087Z","iopub.status.idle":"2025-01-26T19:45:16.875268Z","shell.execute_reply.started":"2025-01-26T19:45:11.148051Z","shell.execute_reply":"2025-01-26T19:45:16.872442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cropped Image Creation","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds2 = load_dataset(\"naver-clova-ix/cord-v2\")\nds = load_dataset(\"naver-clova-ix/cord-v1\")\n\ncord_v1 = ds.copy()\ncord_v2 = ds2.copy()\n\ncord_datasets = {\"cord_v1\": cord_v1, \"cord_v2\": cord_v2}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:45:16.877536Z","iopub.execute_input":"2025-01-26T19:45:16.877993Z","iopub.status.idle":"2025-01-26T19:46:21.62187Z","shell.execute_reply.started":"2025-01-26T19:45:16.877923Z","shell.execute_reply":"2025-01-26T19:46:21.620612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm.auto import tqdm\n\noutput_dir = \"Cropped_images\"\nos.makedirs(output_dir, exist_ok=True)\npadding = 23\ndef crop_and_save_images(dataset, output_dir, dataset_name):\n    for split in tqdm(dataset.keys(), total = len(dataset.keys())):  # Iterate over 'train', 'validation', 'test' splits\n        split_dir = os.path.join(output_dir, split)\n        os.makedirs(split_dir, exist_ok=True)\n\n        for idx, example in tqdm(enumerate(dataset[split]), total = len(dataset[split])):\n            # print(example.keys())\n            image = example['image']\n            annotations = eval(example['ground_truth'])  # Contains bounding boxes\n            width, height = image.size\n            try:\n                min_x1 = float('inf')\n                min_y1 = float('inf')\n                max_x3 = float('-inf')\n                max_y3 = float('-inf')\n\n                for line in annotations[\"valid_line\"]:\n                    for word in line[\"words\"]:\n                        quad = word[\"quad\"]\n                        min_x1 = min(min_x1, quad['x1'])\n                        min_y1 = min(min_y1, quad['y1'])\n                        max_x3 = max(max_x3, quad['x3'])\n                        max_y3 = max(max_y3, quad['y3'])\n\n                box = [min_x1, min_y1, max_x3, max_y3]\n\n                padded_box = [\n                    max(box[0] - (padding-5), 0),  # x1\n                    max(box[1] - padding, 0),  # y1\n                    min(box[2] + padding, width),  # x3\n                    min(box[3] + padding, height)  # y3\n                ]\n                cropped_image = image.crop(padded_box)\n\n                cropped_image_path = os.path.join(split_dir, f\"{dataset_name}_{idx}.png\")\n                cropped_image.save(cropped_image_path)\n\n            except Exception as e:\n                print(f\"Error processing image {image}: {e}\")\n\n\nfor cord_dataset_name, cord_dataset in cord_datasets.items():\n    crop_and_save_images(cord_dataset, output_dir, cord_dataset_name)\n# crop_and_save_images(ds, output_dir, dataset_name)\nprint(f\"All cropped images are saved in: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:46:21.624312Z","iopub.execute_input":"2025-01-26T19:46:21.625057Z","iopub.status.idle":"2025-01-26T19:52:16.378349Z","shell.execute_reply.started":"2025-01-26T19:46:21.625004Z","shell.execute_reply":"2025-01-26T19:52:16.376897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Background Image Creation","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\ndef move_random_images(folder_a, folder_b, n):\n    # Ensure folder_b is writable\n    if not os.path.exists(folder_b):\n        os.makedirs(folder_b)\n    \n    # Check if the folder is in read-only file system, and handle accordingly\n    if \"/input\" in folder_a:\n        print(\"Detected read-only source folder, copying images to working directory...\")\n        writable_folder_a = \"/kaggle/working/images\"\n        if not os.path.exists(writable_folder_a):\n            os.makedirs(writable_folder_a)\n\n        # Copy files to writable folder\n        for file_name in os.listdir(folder_a):\n            file_path = os.path.join(folder_a, file_name)\n            if os.path.isfile(file_path):\n                shutil.copy(file_path, writable_folder_a)\n        folder_a = writable_folder_a\n    \n    # Get list of all images in folder A (writable directory now)\n    all_images = [f for f in os.listdir(folder_a) if os.path.isfile(os.path.join(folder_a, f))]\n    \n    # Ensure we don't select more images than available\n    if n > len(all_images):\n        raise ValueError(f\"Requested {n} images, but only {len(all_images)} available.\")\n\n    # Randomly select n unique images\n    selected_images = random.sample(all_images, n)\n\n    # Define dataset splits (4:1:1 ratio)\n    num_train = int(n * 4 / 6)\n    num_val = int(n * 1 / 6)\n    num_test = n - (num_train + num_val)  # Ensures all images are used\n\n    train_images = selected_images[:num_train]\n    val_images = selected_images[num_train:num_train + num_val]\n    test_images = selected_images[num_train + num_val:]\n\n    # Ensure Folder B and its subdirectories exist\n    for subfolder in [\"train\", \"validation\", \"test\"]:\n        os.makedirs(os.path.join(folder_b, subfolder), exist_ok=True)\n\n    # Move images to respective subfolders\n    for img in train_images:\n        shutil.move(os.path.join(folder_a, img), os.path.join(folder_b, \"train\", img))\n    \n    for img in val_images:\n        shutil.move(os.path.join(folder_a, img), os.path.join(folder_b, \"validation\", img))\n    \n    for img in test_images:\n        shutil.move(os.path.join(folder_a, img), os.path.join(folder_b, \"test\", img))\n\n    print(f\"Moved {n} images to {folder_b} (Train: {num_train}, Val: {num_val}, Test: {num_test})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:52:16.380479Z","iopub.execute_input":"2025-01-26T19:52:16.380935Z","iopub.status.idle":"2025-01-26T19:52:16.39303Z","shell.execute_reply.started":"2025-01-26T19:52:16.38089Z","shell.execute_reply":"2025-01-26T19:52:16.391565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfolder_a = \"/kaggle/input/indoor-scenes-cvpr-2019\"\nif os.path.exists(folder_a):\n    print(os.listdir(folder_a))  # Check if the folder contains any files\nelse:\n    print(f\"Folder not found: {folder_a}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:52:16.39414Z","iopub.execute_input":"2025-01-26T19:52:16.39443Z","iopub.status.idle":"2025-01-26T19:52:16.430279Z","shell.execute_reply.started":"2025-01-26T19:52:16.394401Z","shell.execute_reply":"2025-01-26T19:52:16.429114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_B = \"/kaggle/working/Background\"\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/artstudio\"\nmove_random_images(folder_A, folder_B, 100)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/auditorium\"\nmove_random_images(folder_A, folder_B, 100)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/bathroom\"\nmove_random_images(folder_A, folder_B, 200)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/bedroom\"\nmove_random_images(folder_A, folder_B, 250)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/bookstore\"\nmove_random_images(folder_A, folder_B, 200)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/children_room\"\nmove_random_images(folder_A, folder_B, 250)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/classroom\"\nmove_random_images(folder_A, folder_B, 300)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/corridor\"\nmove_random_images(folder_A, folder_B, 300)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/dining_room\"\nmove_random_images(folder_A, folder_B, 300)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/florist\"\nmove_random_images(folder_A, folder_B, 200)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/kitchen\"\nmove_random_images(folder_A, folder_B, 200)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/livingroom\"\nmove_random_images(folder_A, folder_B, 500)\n\nfolder_A = \"/kaggle/input/indoor-scenes-cvpr-2019/indoorCVPR_09/Images/meeting_room\"\nmove_random_images(folder_A, folder_B, 250)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:52:16.431315Z","iopub.execute_input":"2025-01-26T19:52:16.431698Z","iopub.status.idle":"2025-01-26T19:53:06.153362Z","shell.execute_reply.started":"2025-01-26T19:52:16.431659Z","shell.execute_reply":"2025-01-26T19:53:06.152345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef count_files_in_folder(folder_b):\n    total_files = 0\n    \n    for dirpath, dirnames, filenames in os.walk(folder_b):\n        total_files += len(filenames)  # Count all files in the current directory\n\n    return total_files\n\nfile_count = count_files_in_folder(folder_B)\nprint(f\"Total number of files in {folder_B}: {file_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:53:06.154615Z","iopub.execute_input":"2025-01-26T19:53:06.155014Z","iopub.status.idle":"2025-01-26T19:53:06.169665Z","shell.execute_reply.started":"2025-01-26T19:53:06.154975Z","shell.execute_reply":"2025-01-26T19:53:06.168092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_path = '/kaggle/working/images'\nshutil.rmtree(folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:53:06.172428Z","iopub.execute_input":"2025-01-26T19:53:06.17272Z","iopub.status.idle":"2025-01-26T19:53:06.239934Z","shell.execute_reply.started":"2025-01-26T19:53:06.172694Z","shell.execute_reply":"2025-01-26T19:53:06.238901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Creation","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image, ImageDraw, ImageEnhance\nimport json\n\n\ndef generate_multiple_composites_with_custom_masks_and_distance(\n        cropped_images_dir, output_dir, bg_images_dir,\n        n_images, num_original_images, bg_color_images, bg_images_count,\n        max_crop_images=9, canvas_size=(1000, 1000), min_distance=30, margin=20):\n    \"\"\"\n    Generates multiple composite images with random placement of cropped images on a white canvas,\n    ensuring minimum distance between images. Also generates corresponding masks with unique\n    integer values for each image on a transparent background.\n    \n    Args:\n        cropped_images_dir (str): Directory containing cropped images.\n        output_dir (str): Directory to save the generated images and bounding boxes.\n        bg_images_dir (str): Directory containing background images.\n        n_images (int): Number of composite images to generate.\n        num_original_images (int): Number of original images to add to the composite images.\n        bg_color_images (int): Number of composite images with random background colors.\n        bg_images_count (int): Number of composite images with background images.\n        max_crop_images (int): Maximum number of cropped images to place in each composite image.\n        canvas_size (tuple): Size of the canvas (width, height).\n        min_distance (int): Minimum distance in pixels between placed images.\n        margin (int): Margin (in pixels) to exclude from the mask.\n    \"\"\"\n    count = 0\n    \n    # Create output directories\n    ped_masks_dir = os.path.join(output_dir, \"MasksImages\")\n    png_images_dir = os.path.join(output_dir, \"PNGImages\")\n    os.makedirs(ped_masks_dir, exist_ok=True)\n    os.makedirs(png_images_dir, exist_ok=True)\n    \n    # Load cropped images\n    cropped_image_paths = [\n        os.path.join(cropped_images_dir, f)\n        for f in os.listdir(cropped_images_dir)\n        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n    ]\n    \n    # Load background images if provided\n    bg_images = []\n    if bg_images_dir:\n        bg_images = [\n            os.path.join(bg_images_dir, f)\n            for f in os.listdir(bg_images_dir)\n            if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n        ]\n    \n    # Filter images that fit within the canvas size\n    valid_images = [img_path for img_path in cropped_image_paths if Image.open(img_path).size[0] <= canvas_size[0] and Image.open(img_path).size[1] <= canvas_size[1]]\n    \n    if not valid_images:\n        raise ValueError(\"No valid images found that fit the canvas.\")\n    \n    all_bounding_boxes = []\n    \n    # Generate composite images\n    for i in range(n_images + bg_color_images + bg_images_count):\n        if i % 100 == 0:\n             print(i)\n        \n        # Create a blank canvas\n        if i < n_images:\n            canvas = Image.new(\"RGB\", canvas_size, \"white\")\n        elif i < n_images + bg_color_images:\n            canvas = Image.new(\"RGB\", canvas_size, tuple(random.randint(0, 255) for _ in range(3)))\n        else:\n            canvas = Image.open(random.choice(bg_images)).resize(canvas_size).convert(\"RGB\")  # Ensure RGB mode\n        \n        mask_canvas = Image.new(\"L\", canvas_size, 0)  # Mask canvas (grayscale, starts with 0)\n        bounding_boxes = []\n        mask_id = 1  # Start from 1, as 0 is the background\n        \n        # Select random cropped images\n        selected_images = random.sample(valid_images, min(random.randint(1, max_crop_images), len(valid_images)))\n        \n        # Try placing images without overlap\n        for image_path in selected_images:\n            cropped_image = Image.open(image_path).convert(\"RGBA\")\n            \n            # Apply enhancements (brightness, contrast)\n            if i < (n_images + (bg_color_images // 2)):\n                cropped_image = ImageEnhance.Brightness(cropped_image).enhance(1.4)\n                cropped_image = ImageEnhance.Contrast(cropped_image).enhance(2.0)\n                cropped_image = ImageEnhance.Brightness(cropped_image).enhance(1.2)\n            \n            # Apply random rotation (-5 to 5 degrees)\n            rotated_image = cropped_image.rotate(random.uniform(-5, 5), expand=True)\n            \n            # Apply random scaling (between 0.7 and 1.3x)\n            scale_factor1, scale_factor2 = random.uniform(0.7, 1.3), random.uniform(0.7, 1.3)\n            new_width = min(canvas_size[0], int(rotated_image.size[0] * scale_factor1))\n            new_height = min(canvas_size[1], int(rotated_image.size[1] * scale_factor2))\n            resized_image = rotated_image.resize((new_width, new_height))\n            \n            # Try finding a valid non-overlapping position\n            placed = False\n            for _ in range(100):\n                x_min, y_min = random.randint(0, canvas_size[0] - new_width), random.randint(0, canvas_size[1] - new_height)\n                x_max, y_max = x_min + new_width, y_min + new_height\n                \n                if not any(x_min < box[2] + min_distance and x_max + min_distance > box[0] and y_min < box[3] + min_distance and y_max + min_distance > box[1] for box in bounding_boxes):\n                    canvas.paste(resized_image, (x_min, y_min), resized_image)\n                    for x in range(x_min, x_max):\n                        for y in range(y_min, y_max):\n                            if 0 <= x < canvas_size[0] and 0 <= y < canvas_size[1]:\n                                mask_canvas.putpixel((x, y), mask_id)\n                    bounding_boxes.append((x_min, y_min, x_max, y_max))\n                    mask_id += 1\n                    placed = True\n                    break\n        \n        # Save composite and mask images\n        composite_image_path = os.path.join(png_images_dir, f\"generated_image_{count+1}.jpg\")\n        canvas = canvas.convert(\"RGB\")  # Ensure compatible mode for JPEG\n        canvas.save(composite_image_path, format=\"JPEG\")\n        mask_canvas.save(os.path.join(ped_masks_dir, f\"masked_image_{count+1}.png\"))\n        count += 1\n        \n        all_bounding_boxes.append({\"image\": composite_image_path, \"bounding_boxes\": bounding_boxes})\n    \n    # Save original images\n    for i in range(min(num_original_images, len(valid_images))):\n        original_image_path = random.choice(valid_images)\n        original_image = Image.open(original_image_path).convert(\"RGB\")\n        original_image_output_path = os.path.join(png_images_dir, f\"generated_image_{count+1}.jpg\")\n        original_image.save(original_image_output_path)\n        \n        # Create mask for the original image\n        mask = Image.new(\"L\", original_image.size, 0)\n        ImageDraw.Draw(mask).rectangle([margin, margin, original_image.size[0] - margin, original_image.size[1] - margin], fill=1)\n        mask.save(os.path.join(ped_masks_dir, f\"masked_image_{count+1}.png\"))\n        count += 1\n    \n    # Save bounding boxes to JSON\n    with open(os.path.join(output_dir, \"all_bounding_boxes.json\"), 'w') as f:\n        json.dump(all_bounding_boxes, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:53:06.24146Z","iopub.execute_input":"2025-01-26T19:53:06.241815Z","iopub.status.idle":"2025-01-26T19:53:06.263056Z","shell.execute_reply.started":"2025-01-26T19:53:06.241774Z","shell.execute_reply":"2025-01-26T19:53:06.261948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dir = \"/kaggle/working/Cropped_images/test\"\noutput_dir = \"/kaggle/working/Created_Images/test\"\nbg_images_dir = \"/kaggle/working/Background/test\"\n\ngenerate_multiple_composites_with_custom_masks_and_distance(\n        cropped_images_dir, output_dir, bg_images_dir,\n        n_images = 900, num_original_images = 200, bg_color_images = 500, bg_images_count = 200,\n        max_crop_images=9, canvas_size=(1000, 1000), min_distance=30, margin=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:53:06.26428Z","iopub.execute_input":"2025-01-26T19:53:06.264674Z","execution_failed":"2025-01-26T19:54:13.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dir = \"/kaggle/working/Cropped_images/train\"\noutput_dir = \"/kaggle/working/Created_Images/train\"\nbg_images_dir = \"/kaggle/working/Background/train\"\n\ngenerate_multiple_composites_with_custom_masks_and_distance(\n        cropped_images_dir, output_dir, bg_images_dir,\n        n_images = 900*4, num_original_images = 200*4, bg_color_images = 500*4, bg_images_count = 200*4,\n        max_crop_images=9, canvas_size=(1000, 1000), min_distance=30, margin=20)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dir = \"/kaggle/working/Cropped_images/train\"\noutput_dir = \"/kaggle/working/Created_Images/validation\"\nbg_images_dir = \"/kaggle/working/Background/validation\"\n\ngenerate_multiple_composites_with_custom_masks_and_distance(\n        cropped_images_dir, output_dir, bg_images_dir,\n        n_images = 900, num_original_images = 200, bg_color_images = 500, bg_images_count = 200,\n        max_crop_images=9, canvas_size=(1000, 1000), min_distance=30, margin=20)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git\n%cd vision/references/detection","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"MasksImages\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"MasksImages\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask,\n        hidden_layer,\n        num_classes\n    )\n\n    return model\nimport torch","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import v2\n\ndef get_transform(train):\n    transforms = []\n    if train:\n        # Using RandomShortestSize instead of RandomResize for compatibility\n        transforms.append(v2.RandomShortestSize(min_size=800, max_size=1200))\n        transforms.append(v2.RandomHorizontalFlip(0.5))  # Random horizontal flip with 50% probability\n    # Updated as per warning: Use ToImage and ToDtype instead of deprecated ToTensor\n    transforms.append(v2.ToImage())\n    transforms.append(v2.ToDtype(torch.float32, scale=True))  # Scale=True normalizes the tensor\n    return v2.Compose(transforms)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import utils\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\ndataset_train = PennFudanDataset('/kaggle/working/Created_Images/test', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset_train,\n    batch_size=2,\n    shuffle=True,\n    collate_fn=utils.collate_fn\n)\n\n# For Training\nimages, targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images, targets)  # Returns losses and detections\nprint(output)\n\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)  # Returns predictions\nprint(predictions[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom engine import train_one_epoch, evaluate\nimport utils\n\n# train on the GPU or on the CPU, if a GPU is not available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# use our dataset and defined transformations\ndataset_train = PennFudanDataset('/kaggle/working/Created_Images/train', get_transform(train=True))\ndataset_validation = PennFudanDataset('/kaggle/working/Created_Images/validation', get_transform(train=False))\n\nindices = torch.randperm(len(dataset_train)).tolist()\ndataset_train = torch.utils.data.Subset(dataset_train, indices)\n\nindices = torch.randperm(len(dataset_validation)).tolist()\ndataset_test = torch.utils.data.Subset(dataset_validation, indices)\n\n# define training and validation data loaders\ndata_loader_train = torch.utils.data.DataLoader(\n    dataset_train,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=utils.collate_fn\n)\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=4,\n    shuffle=False,\n    collate_fn=utils.collate_fn\n)\n\n# get the model using our helper function\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\nnum_epochs = 6\n\nfor epoch in range(num_epochs):\n\n    # Evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\n    # Train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n\n    # Update the learning rate\n    lr_scheduler.step()\n\n    # Save the model's state dict to Google Drive (recommended approach)\n    torch.save(model.state_dict(), f\"/kaggle/working/model_{epoch}.pth\")\n\n# Evaluate on the test dataset\nevaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-26T19:54:13.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndataset_test = PennFudanDataset('/kaggle/working/Created_Images/test', get_transform(train=False))\n\nindices = torch.randperm(len(dataset_test)).tolist()\ndataset_test = torch.utils.data.Subset(dataset_test, indices)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=4,\n    shuffle=False,\n    collate_fn=utils.collate_fn\n)\nevaluate(model, data_loader_test, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
